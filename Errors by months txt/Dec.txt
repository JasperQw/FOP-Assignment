[2022-12-02T11:56:00.280] error: Node cpu01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.281] error: Node cpu03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.281] error: Node cpu13 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.282] error: Node cpu12 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.282] error: Node cpu11 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.282] error: Node cpu15 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.282] error: Node cpu05 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.283] error: Node cpu09 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.283] error: Node gpu03 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node cpu10 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node gpu05 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node gpu01 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node cpu08 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node cpu07 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node gpu02 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.284] error: Node gpu04 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.287] error: Node cpu04 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T11:56:00.323] error: Node cpu14 appears to have a different slurm.conf than the slurmctld.  This could cause issues with communication and functionality.  Please review both files and make sure they are the same.  If this is expected ignore, and set DebugFlags=NO_CONF_HASH in your slurm.conf.
[2022-12-02T12:00:56.646] error: Nodes management01 not responding
[2022-12-06T13:40:42.571] error: Invalid qos (long;--job-name=JupLab)
[2022-12-06T13:41:03.056] error: Invalid qos (long,--job-name=Jup)
[2022-12-15T21:35:48.680] _slurm_rpc_complete_job_allocation: JobId=54003 error Job/step already completing or completed
[2022-12-16T02:23:11.814] error: slurm_receive_msgs: [[gpu05.dicc.um.edu.my]:6818] failed: Socket timed out on send/recv operation
[2022-12-16T02:23:31.832] error: slurm_receive_msgs: [[gpu05.dicc.um.edu.my]:6818] failed: Socket timed out on send/recv operation
[2022-12-16T02:23:41.855] error: Nodes gpu05 not responding, setting DOWN
[2022-12-16T02:23:51.858] error: slurm_receive_msgs: [[gpu05.dicc.um.edu.my]:6818] failed: Socket timed out on send/recv operation
[2022-12-16T02:26:09.250] error: Registered PENDING JobId=53962 StepId=53962.extern on node gpu05
[2022-12-16T02:26:09.250] error: Registered PENDING JobId=53962 StepId=53962.batch on node gpu05
